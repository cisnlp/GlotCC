{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from GlotScript import sp\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import string\n",
    "\n",
    "\n",
    "class Filters:\n",
    "    def __init__(self):\n",
    "        self.filters = []\n",
    "\n",
    "    def add_filter(self, filter_func, warning):\n",
    "        self.filters.append((filter_func, warning))\n",
    "\n",
    "    def apply_filters(self, sentence, quality_warnings):\n",
    "        for filter_func, warning in self.filters:\n",
    "            if filter_func(sentence):\n",
    "                quality_warnings.append(warning)\n",
    "        return quality_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CURSED_SUBSTRINGS = [\" №\", \"���\", \"\\\\|\\\\s*$\", \" nr\\\\.$\", \"aute irure dolor \", \" sunt in culpa qui \", \"orem ipsum \", \" quis nostrud \", \" adipisicing \", \" dolore eu \", \" cupidatat \", \"autem vel eum\", \"wisi enim ad\", \" sex \", \" porn \", \"黄色电影\", \"mp3\", \"ownload\", \"Vol\\\\.\", \" Ep\\\\.\", \"Episode\", \" г\\\\.\\\\s*$\", \" кг\\\\.\\\\s*$\", \" шт\\\\.\", \"Develop\", \"Facebook\", \" crusher \", \" xxx \", \" ... ... ... ... ... ... ... ... ...\", \" .... .... .... .... .... .... .... .... ....\", \" [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ]\", \", ..,,? ..,,? ..,,? ..,,?\"]\n",
    "ADULT_SIGNALS = \"caoporn caoprom caopron caoporen caoponrn caoponav caopom caoorn 99re dy888 caopro hezyo re99 4438x zooskool xfplay 7tav xxoo xoxo 52av freexx 91chinese anquye cao97 538porm 87fuli 91pron 91porn 26uuu 4438x 182tv kk4444 777me ae86 91av 720lu yy6080 6080yy qqchub paa97 aiai777 yy4480 videossexo 91free 一级特黄大片 偷拍久久国产视频 日本毛片免费视频观看 久久免费热在线精品 高清毛片在线看 日本毛片高清免费视频 一级黄色录像影片 亚洲男人天堂 久久精品视频在线看 自拍区偷拍亚洲视频 亚洲人成视频在线播放 色姑娘综合站 丁香五月啪啪 在线视频成人社区 亚洲人成视频在线播放 久久国产自偷拍 一本道 大香蕉无码 香港经典三级 亚洲成在人线免费视频 天天色综合网 大香蕉伊人久草 欧美一级高清片 天天鲁夜夜啪视频在线 免费黄片视频在线观看 加比勒久久综合 久草热久草在线视频 韩国三级片大全在线观看 青青草在线视频 美国一级毛片 久草在线福利资源 啪啪啪视频在线观看免费 成人福利视频在线观看 婷婷我去也 老司机在线国产 久久成人视频 手机看片福利永久国产 高清国产偷拍在线 大香蕉在线影院 日本高清免费一本视频 男人的天堂东京热 影音先锋男人资源 五月婷婷开心中文字幕 亚洲香蕉视频在线播放 天天啪久久爱视频精品 超碰久久人人摸人人搞\".split()\n",
    "EXTRA_ADULT_SGINALS = \"日本一级特黄大片 qq的天天中彩票 一本道 一级特黄大片 三级片 下三烂 个老子的 中国福利彩票天天 久久免费热在线精品 久久国产视频 久久精品国产 久久综合久久 乳交 乳波臀浪 五分彩 亚洲男人天堂 亚洲精品 人人摸 人人摸人人 人人操 人人澡 人人爽人人 人人碰 人人碰人人 人人碰免费公开视频 人人碰免费视频 仆街 他奶娘的 他妈 他妈ㄉ王八蛋 他妈地 他妈的 他马的 伦理在线黄影片 伦理在线黄电影 伦理在线黄网影片 伦理在线黄网电影 伦理在线黄网视频 伦理在线黄视频 伦理影片 伦理影片观看 伦理片 伦理片免费 伦理片观看 伦理电影 伦理电影片观看 伦理电影观看 伦理电影观看平台 伦理电影观看网 伦理电影观看网址 伦理电视 伦理视频 伦理视频观看 伦理黄影片 伦理黄影片观看 伦理黄片 伦理黄片网站 伦理黄电影 伦理黄电影片 伦理黄电影观看 伦理黄电视 伦理黄电视片 伦理黄网影片 伦理黄网片 伦理黄网电影 伦理黄网视频 伦理黄网视频观看 伦理黄视频 伦理黄视频片 伦理黄视频网 伦理黄视频观看 伦理黄视频频道 伦理黄频道 你个傻比 你他马的 你全家 你奶奶的 你她马的 你妈的 你娘卡好 你娘咧 你它妈的 你它马的 你是鸡 你是鸭 你马的 做爱 傻比 傻逼 免费人成视频 免费伦理 免费伦理影片 免费伦理视频 免费在线伦理黄影片 免费在线伦理黄电影 免费在线伦理黄网影片 免费在线伦理黄网电影 免费在线伦理黄网视频 免费在线伦理黄视频 免费在线成人视频 免费在线成人黄影片 免费在线成人黄电影 免费在线成人黄网影片 免费在线成人黄网电影 免费在线成人黄网视频 免费在线成人黄视频 免费在线黄影片 免费在线黄电影 免费在线黄网影片 免费在线黄网电影 免费在线黄网视频 免费在线黄视频 免费成人电影 免费成人视频 免费成人视频观看 免费无码 免费色情影片 免费色情电影 免费色情视频 免费视频在线观看 六合彩 册那 军妓 分分彩 北京赛车开奖 午夜电影 午夜福利 卖B 卖比 卖淫 博彩 口交 口肯 吃屎 吹箫 啪啪啪视频 国产av 国产精品 国产自拍 国产自拍片 国产自拍视频 在线av 在线av在线 在线av直播 在线av网址 在线av视频 在线伦理黄影片 在线伦理黄片 在线伦理黄电影 在线伦理黄网影片 在线伦理黄网电影 在线伦理黄网视频 在线伦理黄视频 在线国产 在线大香蕉 在线成人黄影片 在线成人黄片 在线成人黄电影 在线成人黄网影片 在线成人黄网电影 在线成人黄网视频 在线成人黄视频 在线观看中文字幕 在线观看免费 在线黄影片 在线黄片 在线黄电影 在线黄网影片 在线黄网电影 在线黄网视频 在线黄视频 塞你公 塞你娘 塞你母 塞你爸 塞你老师 塞你老母 夜夜啪视频在线观看 大卵子 大卵泡 大发云 大发官网 大发彩票 大发彩票官网 大发快三 大发快三和值 大发快三大小单双 大发快三如何 大发快三官网 大发快三开奖 大发快三开奖结果 大发快三怎么 大发快三怎么看 大发快三是不是 大发快三是什么 大发快三是国家 大发快三计划 大发快三走势图 大发扑克 大发时时彩 大发时时彩开奖 大发时时彩是 大发时时彩计划 大发棋牌 大发游戏 大发游戏官网 大香蕉 大香蕉伊人 大鸡巴 天天中彩票 天天中彩票app 天天中彩票微信 天天中彩票怎么 天天中彩票是 天天中彩票的 天天啪 天天啪在线视频 天天彩票 天天彩票网 天天爱彩票 天天赢彩票 夫妻性生活 奸你 她妈地 她妈的 她马的 妈B 妈个B 妈个比 妈个老比 妈妈的 妈比 妈的 妈的B 妈逼 妓 妓女 妳她妈的 妳妈的 妳娘的 妳老母的 妳马的 娱乐平台开户 娱乐彩票 开奖结果 彩神争霸 彩神争霸大发快三 彩神争霸怎么 彩神争霸是 彩神争霸网站 彩神争霸邀请码 彩票 彩票天天 彩票娱乐 彩票娱乐注册 彩票平台 彩经彩票 微信的天天中彩票 性交 性感写真 性感写真视频 性感美女 性感视频 性爱 性爱视频 情色图片 情色电影 情色网站 情色视频 情色论坛 成人下载 成人书刊 成人动漫 成人动漫网 成人图片 成人在线影片 成人在线电影 成人在线网 成人在线视频 成人在线黄影片 成人在线黄电影 成人在线黄网影片 成人在线黄网电影 成人在线黄网视频 成人在线黄视频 成人小说 成人影城 成人影片 成人影片观看 成人影视 成人文学 成人文学网 成人漫画 成人片 成人电影 成人电影下载 成人电影免费 成人电影在线 成人电影播放 成人电影片 成人电影片观看 成人电影观看 成人电影观看免费 成人电影观看平台 成人电影观看网 成人电影观看网址 成人电视 成人秀 成人网站 成人自拍 成人视频 成人视频免费网站 成人视频啪啪啪 成人视频在线 成人视频在线观看 成人视频平台 成人视频播放 成人视频片 成人视频秀 成人视频网 成人视频自拍 成人视频观看 成人视频观看免费 成人视频观看平台 成人视频观看网站 成人视频频道 成人论坛 成人频道 成人黄影片 成人黄影片观看 成人黄片 成人黄片网站 成人黄电影 成人黄电影片 成人黄电影观看 成人黄电视 成人黄电视片 成人黄网影片 成人黄网片 成人黄网电影 成人黄网视频 成人黄网视频观看 成人黄视频 成人黄视频片 成人黄视频网 成人黄视频观看 成人黄视频频道 成人黄频道 无码 无码av 无码一区二区三区 无码不卡高清免费 无码不卡高清免费v 无码中文字幕 无码国产自拍 日本一本道 日本毛片免费视频观看 日韩av 时时彩 时时彩计划 淫秽 激情小说 激情裸聊 激情裸舞 激情视频 热久久精品 热在线精品 热这里只有精品 男人的天堂 真人性生活直播 真人性生活视频 真人性直播 真人性行为直播 真人性行为视频 真人性视频 真人秀 真人秀视频 真人色情 真人色情视频 真人裸聊 真人裸舞 真人视频 真人视频秀 神彩争霸 福利彩票 福利视频 精品一区二区三区 精品国产 美女裸聊 美女裸舞 自拍视频 色情 色情图片 色情影片 色情电影 色情电影观看 色情电视 色情网 色情网站 色情视频 色情视频在线 色情视频在线观看 色情视频网 色情视频频道 色情频道 色电影 色网 色网站 色视频 色视频在线 色视频网站 赌 赌债 赌博 赌城 赌局 赌徒 赌桌 赌注 赌王 赌瘾 赌盘 赌赛 赌钱 赌鬼 重庆时时彩 重庆时时彩杀 露点 高清无码 黄影片 黄片 黄片网站 黄电影 黄电影片 黄电视 黄电视片 黄网 黄网影片 黄网片 黄网电影 黄网视频 黄色 黄色录像 黄色录像影片 黄色录像片 黄色录像片影片 黄色录像片电影 黄色录像片网影片 黄色录像片网电影 黄色录像片网视频 黄色录像片视频 黄色录像电影 黄色录像网影片 黄色录像网电影 黄色录像网视频 黄色录像视频 黄视频 黄视频片 黄视频网\".split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "POLICY_SUBSTRINGS = [\n",
    "    \"terms of use\",\n",
    "    \"privacy policy\",\n",
    "    \"cookie policy\",\n",
    "    \"uses cookies\",\n",
    "    \"use of cookies\",\n",
    "    \"use cookies\",\n",
    "]\n",
    "\n",
    "\n",
    "def list_case_filter(sentence):\n",
    "    tokens = sentence.split()\n",
    "    capital_tokens = [token for token in tokens if token[0].isupper() or all(char.isdigit() or char in string.punctuation for char in token)]\n",
    "    warning = len(tokens) >= 12 and (len(capital_tokens) / len(tokens)) > 0.5\n",
    "    return warning\n",
    "\n",
    "\n",
    "def danger_chars_filter(sentence):\n",
    "    danger_chars_count = sum(1 for char in sentence if char in '0123456789{}+/()>')\n",
    "    warning = (danger_chars_count / len(sentence)) > 0.2\n",
    "    return warning\n",
    "\n",
    "\n",
    "def cursedness_filter(sentence):\n",
    "    warning = any(curse in sentence for curse in CURSED_SUBSTRINGS)\n",
    "    return warning\n",
    "\n",
    "def adult_filter(sentence):\n",
    "    warning = any(curse in sentence for curse in ADULT_SIGNALS + EXTRA_ADULT_SGINALS)\n",
    "    return warning\n",
    "\n",
    "def detect_long_words(sentence, max_chars=100):\n",
    "    words = sentence.split()\n",
    "    long_words = [word for word in words if len(word) > max_chars]\n",
    "    return len(long_words) > 0\n",
    "\n",
    "def detect_js_warning(sentence):\n",
    "\n",
    "    # Check if \"Javascript\" is present in the text\n",
    "    if 'Javascript' in sentence or 'JavaScript' in sentence:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_lorem_ipsum(sentence):\n",
    "    # Convert text to lowercase to make the search case-insensitive\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Check if the phrase \"lorem ipsum\" is present in the text\n",
    "    if \"lorem ipsum\" in sentence:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_curly_bracket(sentence):\n",
    "    # Check if the curly bracket '{' is present in the text\n",
    "    if \"{\" in sentence:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_policy(text):\n",
    "    text_lower = text.lower()\n",
    "    for substring in POLICY_SUBSTRINGS:\n",
    "        if substring in text_lower:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GopherRepetitionFilter():\n",
    "    \n",
    "    \"\"\"Check if there is repeated content in the input text. Excessive\n",
    "    repetition is often linked with uninformative content and can be used to\n",
    "    determine whether it is low-quality text. This function implements\n",
    "    \"Repetition Removal\" as described in Gopher_.\n",
    "\n",
    "    .. _Gopher: https://arxiv.org/abs/2112.11446\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_n_grams(words: List[str], n: int) -> List[str]:\n",
    "        return [\" \".join(words[i : i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "    @staticmethod\n",
    "    def find_top_duplicate(x: List[str]) -> int:\n",
    "        counter = Counter()\n",
    "        for element in x:\n",
    "            counter[element] += 1\n",
    "        top_n_gram = counter.most_common(1)[0]\n",
    "        return len(top_n_gram[0]) * top_n_gram[1]\n",
    "\n",
    "\n",
    "    def filter(self, text, top_n_grams):\n",
    "        \n",
    "        text = self.add_space_between_numbers(text)\n",
    "        words = text.split(' ')\n",
    "\n",
    "        for n, n_frac in top_n_grams:\n",
    "            n_grams = self.get_n_grams(words, n)\n",
    "            if not n_grams:\n",
    "                continue\n",
    "            top_char_length = self.find_top_duplicate(n_grams)\n",
    "            if top_char_length / len(text) > n_frac:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_space_between_numbers(numbers):\n",
    "        result = ''\n",
    "        i = 0\n",
    "        while i < len(numbers):\n",
    "            if numbers[i].isdigit():\n",
    "                result += numbers[i]\n",
    "                i += 1\n",
    "                while i < len(numbers) and numbers[i].isdigit():\n",
    "                    result += numbers[i]\n",
    "                    i += 1\n",
    "                if i < len(numbers) and not numbers[i].isdigit():\n",
    "                    result += ' '\n",
    "            else:\n",
    "                result += numbers[i]\n",
    "                if i+1 < len(numbers) and numbers[i+1].isdigit():\n",
    "                    result += ' '\n",
    "                i += 1\n",
    "        return result\n",
    "    \n",
    "    def filter_para(self, text):\n",
    "        \n",
    "        return self.filter(text, ((2, 0.2), (3, 0.18), (4, 0.16)))\n",
    "    \n",
    "    def filter_sent(self, text):\n",
    "        \n",
    "        sents = [s for s in text.split('\\n') if len(s.split(' ')) > 20]\n",
    "        warning = any([self.filter(s, ((1, 0.5), (2, 0.3))) for s in sents])\n",
    "        \n",
    "        return warning\n",
    "\n",
    "gopher_repetition = GopherRepetitionFilter().filter_para\n",
    "gopher_repetition_sent = GopherRepetitionFilter().filter_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filters = Filters()\n",
    "filters.add_filter(list_case_filter, \"list_case\")\n",
    "filters.add_filter(danger_chars_filter, \"danger_chars\")\n",
    "filters.add_filter(cursedness_filter, \"cursed_regex\")\n",
    "filters.add_filter(adult_filter, \"adult_signals\")\n",
    "filters.add_filter(detect_long_words, \"long_word\")\n",
    "filters.add_filter(detect_policy, \"detect_policy\")\n",
    "filters.add_filter(gopher_repetition, \"repetition\")\n",
    "filters.add_filter(gopher_repetition_sent, \"repetition_sent\")\n",
    "filters.add_filter(detect_js_warning, \"js_warning\")\n",
    "filters.add_filter(detect_lorem_ipsum, \"lorem_ipsum\")\n",
    "filters.add_filter(detect_curly_bracket, \"curly_bracket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DomainLabeler:\n",
    "    def __init__(self):\n",
    "        self.domain_groups = {}\n",
    "    \n",
    "    def add_domain_group(self, label, domains):\n",
    "        self.domain_groups[label] = domains\n",
    "    \n",
    "    def apply(self, uri, categories):\n",
    "        domain = urlparse(uri).netloc\n",
    "        for label, domains in self.domain_groups.items():\n",
    "            if any(domain.endswith(d) for d in domains):\n",
    "                categories.append(label)\n",
    "                break\n",
    "        return categories\n",
    "\n",
    "\n",
    "labeler = DomainLabeler()\n",
    "labeler.add_domain_group(\"religious\", [\"bible.com\", \"ebible.org\", \"png.bible\", \"jw.org\", \"wol.jw.org\", \"breakeveryyoke.com\", \"scriptureearth.org\", \"live.bible.is\", \"bible.is\", \"faithcomesbyhearing.com\", \"download.sabda.org\", \"sabda.org\", \"alkitab.mobi\", \"biblerevelation.org\", \"gospelgo.com\", \"mykitabsuci.org\", \"aboriginalbibles.org.au\", \"wikiislam.net\", \"stepbible.org\", \"e-alkitab.org\"])\n",
    "labeler.add_domain_group(\"wikipedia\", [\"wikipedia.org\", \"wikimedia.org\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class PIIReplacer:\n",
    "    def __init__(\n",
    "        self, regex: str, replacements, validator\n",
    "    ):\n",
    "        self.regex: re.Pattern = re.compile(regex)\n",
    "        self.replacements = (\n",
    "            replacements\n",
    "            if type(replacements) is tuple\n",
    "            else (tuple(replacements) if not isinstance(replacements, str) else (replacements,))\n",
    "        )\n",
    "        self.validator = validator  # extra validation for a match\n",
    "        self._replace_i = 0\n",
    "\n",
    "    def replace(self, text: str):\n",
    "        def get_replacement(matchobj):\n",
    "            if self.validator and not self.validator(matchobj.group(0)):\n",
    "                # not a valid match. replace with itself\n",
    "                return matchobj.group(0)\n",
    "            replacement = self.replacements[self._replace_i]\n",
    "            self._replace_i = (self._replace_i + 1) % len(self.replacements)\n",
    "            return replacement\n",
    "\n",
    "        return self.regex.sub(get_replacement, text)\n",
    "\n",
    "\n",
    "def public_ip_validator(ip, public_only: bool = True) -> bool:\n",
    "    try:\n",
    "        ip = ipaddress.ip_address(ip)\n",
    "        return not public_only or ip.is_global\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "class PIIFormatter():\n",
    "    \"\"\"\n",
    "    Replaces email addresses and ip addresses in the document text.\n",
    "    Args:\n",
    "        remove_emails: Replace email addresses\n",
    "        remove_ips: Replace IP addresses\n",
    "        only_remove_public_ips: by default we only replace public (and thus PII) IPs\n",
    "        email_replacement: tuple of strings to use as replacement. They will be used in a circular way\n",
    "        ip_replacement same as email_replacement but for IP addresses\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"📞 PII\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        remove_emails: bool = True,\n",
    "        remove_ips: bool = True,\n",
    "        only_remove_public_ips: bool = True,\n",
    "        # example.com/org are actually maintained as an example\n",
    "        email_replacement  = (\"email@example.com\", \"firstname.lastname@example.org\"),\n",
    "        # randomly generated list of ips. they did not respond to ping requests at the time the list was created\n",
    "        ip_replacement = (\n",
    "            \"22.214.171.124\",\n",
    "            \"126.96.36.199\",\n",
    "            \"188.8.131.52\",\n",
    "            \"184.108.40.206\",\n",
    "            \"220.127.116.11\",\n",
    "            \"18.104.22.168\",\n",
    "        ),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.remove_emails = remove_emails\n",
    "        self.remove_ips = remove_ips\n",
    "\n",
    "        self.emails_replacer = PIIReplacer(\n",
    "            r\"\\b[A-Za-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[A-Za-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:(?:[A-Za-z0-9](?:[\"\n",
    "            r\"A-Za-z0-9-]*[A-Za-z0-9])?\\.)+[A-Za-z0-9](?:[A-Za-z0-9-]*[A-Za-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[\"\n",
    "            r\"01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[A-Za-z0-9-]*[A-Za-z0-9]:)])\",\n",
    "            email_replacement,\n",
    "            None\n",
    "        )\n",
    "\n",
    "        self.ip_replacer = PIIReplacer(\n",
    "            r\"(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\",\n",
    "            validator=partial(public_ip_validator, public_only=only_remove_public_ips),\n",
    "            replacements=ip_replacement,\n",
    "        )\n",
    "\n",
    "    def format(self, text: str) -> str:\n",
    "        if self.remove_emails:\n",
    "            text = self.emails_replacer.replace(text)\n",
    "        if self.remove_ips:\n",
    "            text = self.ip_replacer.replace(text)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "pii_format = PIIFormatter().format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def detect_script(text, lang):\n",
    "    \n",
    "    main_script, percentage, details = sp(text)\n",
    "    if lang == 'jpn':\n",
    "        main_script = 'Jpan'\n",
    "        percentage = details['details'].get('Hani', 0) + details['details'].get('Hira', 0) + details['details'].get('Kana', 0)\n",
    "        \n",
    "    return main_script, percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_json(json_data, lang, script):\n",
    "    \n",
    "    # pii\n",
    "    json_data[\"content\"] = pii_format(json_data[\"content\"])\n",
    "    content = json_data[\"content\"]\n",
    "    \n",
    "    warc_headers = json_data[\"warc_headers\"]\n",
    "    uri = warc_headers[\"warc-target-uri\"]\n",
    "\n",
    "    metadata = json_data[\"metadata\"]\n",
    "    \n",
    "    quality_warnings = metadata.get(\"quality_warnings\", [])\n",
    "    categories = metadata.get(\"categories\", [])\n",
    "    \n",
    "    if categories is None:\n",
    "        categories = []\n",
    "\n",
    "    if quality_warnings is None:\n",
    "        quality_warnings = []\n",
    "        \n",
    "    quality_warnings = filters.apply_filters(content, quality_warnings)\n",
    "    categories = labeler.apply(uri, categories)\n",
    "\n",
    "    metadata[\"quality_warnings\"] = sorted(set(quality_warnings))\n",
    "    metadata[\"categories\"] = sorted(set(categories))\n",
    "\n",
    "    \n",
    "    # run script identification\n",
    "    script_label, script_percentage = detect_script(content, lang)\n",
    "    metadata[\"script\"] = {\"label\": script_label, \"percentage\": float(\"{:.2f}\".format(script_percentage))}\n",
    "\n",
    "    \n",
    "    ## lang identification consistency\n",
    "    \n",
    "    label = metadata['identification']['label']\n",
    "    sent_labels = [int(isent['label']==label) for isent in metadata['sentence_identifications'] if isinstance(isent, dict)]\n",
    "    metadata['identification_consistency'] = {\"percentage\": float(\"{:.2f}\".format(sum(sent_labels)/len(sent_labels))), 'num_sents': len(sent_labels)}\n",
    "    \n",
    "    \n",
    "    return json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Hani_nonspace_percentage(text):\n",
    "    # Split the text into words\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    # Count the number of words with 10 or more characters\n",
    "    long_words_length = sum(len(word) for word in words if len(word) >= 20)\n",
    "    \n",
    "    # Calculate the percentage\n",
    "    percentage = long_words_length / len(text)\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def post_annotation_filter(json_data, lang, script):\n",
    "    \n",
    "    metadata = json_data['metadata']\n",
    "    \n",
    "    if 'wikipedia' in metadata[\"categories\"]:\n",
    "        quality = set(metadata['quality_warnings']) - set([\"short_sentences\",\"header\", \"footer\", \"tiny\", \"long_word\", \"repetition\", \"repetition_sent\"])\n",
    "    \n",
    "    # tiny for oscar is based on 5 sentences, we decrease it to 3\n",
    "    if 'tiny' in metadata['quality_warnings']:\n",
    "        \n",
    "        if metadata['identification_consistency']['num_sents'] >= 3:\n",
    "            metadata['quality_warnings'].remove('tiny')\n",
    "    \n",
    "    \n",
    "    if metadata['script']['label'] != script and script not in ['Hani', 'Hans', 'Hant']:\n",
    "        return False\n",
    "    \n",
    "    if script in ['Hani', 'Hans', 'Hant'] and metadata['script']['label'] not in ['Hani', 'Hans', 'Hant']:\n",
    "        return False\n",
    "    \n",
    "    if metadata['script']['percentage'] < 0.9:\n",
    "        return False\n",
    "    \n",
    "    if lang != 'und' and metadata['identification_consistency']['percentage'] < 0.6:\n",
    "        return False\n",
    "\n",
    "    if script in ['Hani', 'Jpan']:\n",
    "        \n",
    "        if Hani_nonspace_percentage(json_data['content']) < 0.3:\n",
    "            metadata['quality_warnings'].append('hani_list_case')\n",
    "          \n",
    "        \n",
    "        # for chinese and japanse do not remove tiny. \n",
    "        quality = set(metadata['quality_warnings']) - set([\"short_sentences\",\"header\", \"footer\", \"tiny\", \"long_word\", \"repetition\", \"repetition_sent\"])\n",
    "\n",
    "\n",
    "    if script in ['Latn', 'Cyrl', 'Arab', 'Grek', 'Hebr', 'Deva', 'Beng']:\n",
    "        quality = set(metadata['quality_warnings']) - set([\"short_sentences\",\"header\", \"footer\"])\n",
    "\n",
    "    else:\n",
    "        quality = set(metadata['quality_warnings']) - set([\"short_sentences\",\"header\", \"footer\", \"long_word\", \"repetition\", \"repetition_sent\"])\n",
    "    \n",
    "    \n",
    "    # quality = quality - set(['curly_bracket'])\n",
    "    \n",
    "    if len(quality)!=0:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Batch size\n",
    "batch_size = 2000\n",
    "\n",
    "\n",
    "# Function to process JSON data\n",
    "def process_json_file(file_path):\n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    filter_file_path = os.path.join(filter_dir, os.path.basename(file_path))\n",
    "\n",
    "    lang =  os.path.basename(file_path).split('-')[0]\n",
    "    script = os.path.basename(file_path).split('-')[-1].split('_')[0]\n",
    "    \n",
    "    if lang == 'multi':\n",
    "        return None\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as input_file, open(output_file_path, 'w', encoding='utf-8') as output_file, open(filter_file_path, 'w', encoding='utf-8') as filter_file:\n",
    "        batch = []\n",
    "        batch_filter = []\n",
    "        for line in tqdm(input_file, desc=f'Processing {file_path}'):\n",
    "            # Parse the line as JSON\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except:\n",
    "                continue \n",
    "            # Process the JSON object\n",
    "            processed_data = process_json(data, lang, script)\n",
    "            # Add processed data to the batch\n",
    "            batch.append(processed_data)\n",
    "\n",
    "            if post_annotation_filter(processed_data, lang, script):\n",
    "                batch_filter.append(processed_data)\n",
    "\n",
    "            # If the batch is full, write it to the output file\n",
    "            if len(batch) == batch_size:\n",
    "                for item in batch:\n",
    "                    output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                batch = []\n",
    "\n",
    "            # If the batch_filter is full, write it to the output file\n",
    "            if len(batch_filter) == batch_size:\n",
    "                for item in batch_filter:\n",
    "                    filter_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                batch_filter = []\n",
    "\n",
    "        # Write remaining data in the last batch to the output file\n",
    "        for item in batch:\n",
    "            output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        # Write remaining data in the last filter_batch to the output file\n",
    "        for item in batch_filter:\n",
    "            filter_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_cores = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to filter paths based on file size\n",
    "def filter_paths_by_size(paths, max_size_mb):\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n",
    "    filtered_paths = [path for path in paths if os.path.isfile(path) and os.path.getsize(path) < max_size_bytes]\n",
    "    return filtered_paths\n",
    "\n",
    "input_dir = 'res/corpus/'\n",
    "output_dir = 'res/annotation/'\n",
    "filter_dir = 'res/filter/'\n",
    "\n",
    "# Get list of input files\n",
    "input_files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.jsonl')]\n",
    "input_files = sorted(input_files, key=os.path.getsize)\n",
    "\n",
    "input_files_100mgb = filter_paths_by_size(input_files, 100)\n",
    "\n",
    "rest_input_files = list(set(input_files) - set(input_files_100mgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of processes to run in parallel\n",
    "num_processes = min(len(input_files_100mgb), num_cores)\n",
    "\n",
    "# Create a pool of processes\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    # Map the processing function to each input file\n",
    "    pool.map(process_json_file, input_files_100mgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "\n",
    "def process_batch(lines, lang, script):\n",
    "    \n",
    "    batch = []\n",
    "    batch_filter = []\n",
    "    for line in lines:\n",
    "        # Parse the line as JSON\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except:\n",
    "            continue \n",
    "        # Process the JSON object\n",
    "        processed_data = process_json(data, lang, script)\n",
    "        # Add processed data to the batch\n",
    "        batch.append(processed_data)\n",
    "\n",
    "        if post_annotation_filter(processed_data, lang, script):\n",
    "            batch_filter.append(processed_data)\n",
    "            \n",
    "    return batch, batch_filter\n",
    "\n",
    "\n",
    "def check_and_remove_empty_file(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file at file_path is empty and remove it if it is.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the file to be checked and potentially removed.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path) and os.path.getsize(file_path) == 0:\n",
    "        os.remove(file_path)\n",
    "\n",
    "\n",
    "def process_chunk(start_line, end_line, input_file_path, chunk_index, batch_size=2000):\n",
    "    \n",
    "    print(\"chunk_index\", chunk_index)\n",
    "    \n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(input_file_path).replace('.jsonl', f'_{chunk_index}.jsonl'))\n",
    "    filter_file_path = os.path.join(filter_dir, os.path.basename(input_file_path).replace('.jsonl', f'_{chunk_index}.jsonl'))\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile, open(filter_file_path, 'w', encoding='utf-8') as filterfile:\n",
    "        \n",
    "        lang =  os.path.basename(input_file_path).split('-')[0]\n",
    "        script = os.path.basename(input_file_path).split('-')[-1].split('_')[0]\n",
    "\n",
    "        \n",
    "        current_line = 0\n",
    "        lines = []\n",
    "        \n",
    "        for line in infile:\n",
    "            if current_line >= start_line:\n",
    "                lines.append(line)\n",
    "            current_line += 1\n",
    "\n",
    "            if current_line >= end_line:\n",
    "                break\n",
    "\n",
    "            if len(lines) == batch_size:\n",
    "                # Process the lines (replace this with your actual processing function)\n",
    "                processed_lines, processed_filtered_lines  = process_batch(lines, lang, script)\n",
    "                                \n",
    "                for item in processed_lines:\n",
    "                    outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                for item in processed_filtered_lines:\n",
    "                    filterfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                lines = []\n",
    "\n",
    "        # Process any remaining lines in the last batch\n",
    "        if lines:\n",
    "            processed_lines, processed_filtered_lines  = process_batch(lines, lang, script)\n",
    "\n",
    "            for item in processed_lines:\n",
    "                outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            for item in processed_filtered_lines:\n",
    "                filterfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "    check_and_remove_empty_file(output_file_path)\n",
    "    check_and_remove_empty_file(filter_file_path)\n",
    "    \n",
    "    return True\n",
    "        \n",
    "        \n",
    "        \n",
    "# def count_lines(file_path):\n",
    "#     result = subprocess.run(['sed', '-n', '$=', file_path], capture_output=True, text=True)\n",
    "#     return int(result.stdout.strip())\n",
    "              \n",
    "\n",
    "def count_lines(filename):\n",
    "    count = 0\n",
    "    with open(filename, 'rb') as f:\n",
    "        while chunk := f.read(1024*1024*1024):\n",
    "            count += chunk.count(b'\\n')\n",
    "    return count        \n",
    "\n",
    "def chunkify(file_path, num_chunks):\n",
    "    # with open(file_path, 'r') as infile:\n",
    "    #    total_lines = sum(1 for _ in infile)\n",
    "    print(file_path)\n",
    "    total_lines = count_lines(file_path)\n",
    "    chunk_size = total_lines // num_chunks\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start_line = i * chunk_size\n",
    "        end_line = start_line + chunk_size if i != num_chunks - 1 else total_lines\n",
    "        chunks.append((start_line, end_line))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def parallel(input_file, num_cores):\n",
    "    \n",
    "    print(\"start\", input_file)\n",
    "    chunks = chunkify(input_file, num_cores)\n",
    "    print(\"chunks\", len(chunks))\n",
    "    \n",
    "    processes = []\n",
    "    for chunk_index, (start_line, end_line) in enumerate(chunks):\n",
    "        p = mp.Process(target=process_chunk, args=(start_line, end_line, input_file, chunk_index))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rest_input_files = sorted(rest_input_files, key=os.path.getsize)\n",
    "\n",
    "for r in rest_input_files:\n",
    "    parallel(r, num_cores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lid",
   "language": "python",
   "name": "lid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}