{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from GlotScript import sp\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import string\n",
    "\n",
    "\n",
    "class Filters:\n",
    "    def __init__(self):\n",
    "        self.filters = []\n",
    "\n",
    "    def add_filter(self, filter_func, warning):\n",
    "        self.filters.append((filter_func, warning))\n",
    "\n",
    "    def apply_filters(self, sentence, quality_warnings):\n",
    "        for filter_func, warning in self.filters:\n",
    "            if filter_func(sentence):\n",
    "                quality_warnings.append(warning)\n",
    "        return quality_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CURSED_SUBSTRINGS = [\" â„–\", \"ï¿½ï¿½ï¿½\", \"\\\\|\\\\s*$\", \" nr\\\\.$\", \"aute irure dolor \", \" sunt in culpa qui \", \"orem ipsum \", \" quis nostrud \", \" adipisicing \", \" dolore eu \", \" cupidatat \", \"autem vel eum\", \"wisi enim ad\", \" sex \", \" porn \", \"é»„è‰²ç”µå½±\", \"mp3\", \"ownload\", \"Vol\\\\.\", \" Ep\\\\.\", \"Episode\", \" Ð³\\\\.\\\\s*$\", \" ÐºÐ³\\\\.\\\\s*$\", \" ÑˆÑ‚\\\\.\", \"Develop\", \"Facebook\", \" crusher \", \" xxx \", \" ... ... ... ... ... ... ... ... ...\", \" .... .... .... .... .... .... .... .... ....\", \" [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ]\", \", ..,,? ..,,? ..,,? ..,,?\"]\n",
    "ADULT_SIGNALS = \"caoporn caoprom caopron caoporen caoponrn caoponav caopom caoorn 99re dy888 caopro hezyo re99 4438x zooskool xfplay 7tav xxoo xoxo 52av freexx 91chinese anquye cao97 538porm 87fuli 91pron 91porn 26uuu 4438x 182tv kk4444 777me ae86 91av 720lu yy6080 6080yy qqchub paa97 aiai777 yy4480 videossexo 91free ä¸€çº§ç‰¹é»„å¤§ç‰‡ å·æ‹ä¹…ä¹…å›½äº§è§†é¢‘ æ—¥æœ¬æ¯›ç‰‡å…è´¹è§†é¢‘è§‚çœ‹ ä¹…ä¹…å…è´¹çƒ­åœ¨çº¿ç²¾å“ é«˜æ¸…æ¯›ç‰‡åœ¨çº¿çœ‹ æ—¥æœ¬æ¯›ç‰‡é«˜æ¸…å…è´¹è§†é¢‘ ä¸€çº§é»„è‰²å½•åƒå½±ç‰‡ äºšæ´²ç”·äººå¤©å ‚ ä¹…ä¹…ç²¾å“è§†é¢‘åœ¨çº¿çœ‹ è‡ªæ‹åŒºå·æ‹äºšæ´²è§†é¢‘ äºšæ´²äººæˆè§†é¢‘åœ¨çº¿æ’­æ”¾ è‰²å§‘å¨˜ç»¼åˆç«™ ä¸é¦™äº”æœˆå•ªå•ª åœ¨çº¿è§†é¢‘æˆäººç¤¾åŒº äºšæ´²äººæˆè§†é¢‘åœ¨çº¿æ’­æ”¾ ä¹…ä¹…å›½äº§è‡ªå·æ‹ ä¸€æœ¬é“ å¤§é¦™è•‰æ— ç  é¦™æ¸¯ç»å…¸ä¸‰çº§ äºšæ´²æˆåœ¨äººçº¿å…è´¹è§†é¢‘ å¤©å¤©è‰²ç»¼åˆç½‘ å¤§é¦™è•‰ä¼Šäººä¹…è‰ æ¬§ç¾Žä¸€çº§é«˜æ¸…ç‰‡ å¤©å¤©é²å¤œå¤œå•ªè§†é¢‘åœ¨çº¿ å…è´¹é»„ç‰‡è§†é¢‘åœ¨çº¿è§‚çœ‹ åŠ æ¯”å‹’ä¹…ä¹…ç»¼åˆ ä¹…è‰çƒ­ä¹…è‰åœ¨çº¿è§†é¢‘ éŸ©å›½ä¸‰çº§ç‰‡å¤§å…¨åœ¨çº¿è§‚çœ‹ é’é’è‰åœ¨çº¿è§†é¢‘ ç¾Žå›½ä¸€çº§æ¯›ç‰‡ ä¹…è‰åœ¨çº¿ç¦åˆ©èµ„æº å•ªå•ªå•ªè§†é¢‘åœ¨çº¿è§‚çœ‹å…è´¹ æˆäººç¦åˆ©è§†é¢‘åœ¨çº¿è§‚çœ‹ å©·å©·æˆ‘åŽ»ä¹Ÿ è€å¸æœºåœ¨çº¿å›½äº§ ä¹…ä¹…æˆäººè§†é¢‘ æ‰‹æœºçœ‹ç‰‡ç¦åˆ©æ°¸ä¹…å›½äº§ é«˜æ¸…å›½äº§å·æ‹åœ¨çº¿ å¤§é¦™è•‰åœ¨çº¿å½±é™¢ æ—¥æœ¬é«˜æ¸…å…è´¹ä¸€æœ¬è§†é¢‘ ç”·äººçš„å¤©å ‚ä¸œäº¬çƒ­ å½±éŸ³å…ˆé”‹ç”·äººèµ„æº äº”æœˆå©·å©·å¼€å¿ƒä¸­æ–‡å­—å¹• äºšæ´²é¦™è•‰è§†é¢‘åœ¨çº¿æ’­æ”¾ å¤©å¤©å•ªä¹…ä¹…çˆ±è§†é¢‘ç²¾å“ è¶…ç¢°ä¹…ä¹…äººäººæ‘¸äººäººæž\".split()\n",
    "EXTRA_ADULT_SGINALS = \"æ—¥æœ¬ä¸€çº§ç‰¹é»„å¤§ç‰‡ qqçš„å¤©å¤©ä¸­å½©ç¥¨ ä¸€æœ¬é“ ä¸€çº§ç‰¹é»„å¤§ç‰‡ ä¸‰çº§ç‰‡ ä¸‹ä¸‰çƒ‚ ä¸ªè€å­çš„ ä¸­å›½ç¦åˆ©å½©ç¥¨å¤©å¤© ä¹…ä¹…å…è´¹çƒ­åœ¨çº¿ç²¾å“ ä¹…ä¹…å›½äº§è§†é¢‘ ä¹…ä¹…ç²¾å“å›½äº§ ä¹…ä¹…ç»¼åˆä¹…ä¹… ä¹³äº¤ ä¹³æ³¢è‡€æµª äº”åˆ†å½© äºšæ´²ç”·äººå¤©å ‚ äºšæ´²ç²¾å“ äººäººæ‘¸ äººäººæ‘¸äººäºº äººäººæ“ äººäººæ¾¡ äººäººçˆ½äººäºº äººäººç¢° äººäººç¢°äººäºº äººäººç¢°å…è´¹å…¬å¼€è§†é¢‘ äººäººç¢°å…è´¹è§†é¢‘ ä»†è¡— ä»–å¥¶å¨˜çš„ ä»–å¦ˆ ä»–å¦ˆã„‰çŽ‹å…«è›‹ ä»–å¦ˆåœ° ä»–å¦ˆçš„ ä»–é©¬çš„ ä¼¦ç†åœ¨çº¿é»„å½±ç‰‡ ä¼¦ç†åœ¨çº¿é»„ç”µå½± ä¼¦ç†åœ¨çº¿é»„ç½‘å½±ç‰‡ ä¼¦ç†åœ¨çº¿é»„ç½‘ç”µå½± ä¼¦ç†åœ¨çº¿é»„ç½‘è§†é¢‘ ä¼¦ç†åœ¨çº¿é»„è§†é¢‘ ä¼¦ç†å½±ç‰‡ ä¼¦ç†å½±ç‰‡è§‚çœ‹ ä¼¦ç†ç‰‡ ä¼¦ç†ç‰‡å…è´¹ ä¼¦ç†ç‰‡è§‚çœ‹ ä¼¦ç†ç”µå½± ä¼¦ç†ç”µå½±ç‰‡è§‚çœ‹ ä¼¦ç†ç”µå½±è§‚çœ‹ ä¼¦ç†ç”µå½±è§‚çœ‹å¹³å° ä¼¦ç†ç”µå½±è§‚çœ‹ç½‘ ä¼¦ç†ç”µå½±è§‚çœ‹ç½‘å€ ä¼¦ç†ç”µè§† ä¼¦ç†è§†é¢‘ ä¼¦ç†è§†é¢‘è§‚çœ‹ ä¼¦ç†é»„å½±ç‰‡ ä¼¦ç†é»„å½±ç‰‡è§‚çœ‹ ä¼¦ç†é»„ç‰‡ ä¼¦ç†é»„ç‰‡ç½‘ç«™ ä¼¦ç†é»„ç”µå½± ä¼¦ç†é»„ç”µå½±ç‰‡ ä¼¦ç†é»„ç”µå½±è§‚çœ‹ ä¼¦ç†é»„ç”µè§† ä¼¦ç†é»„ç”µè§†ç‰‡ ä¼¦ç†é»„ç½‘å½±ç‰‡ ä¼¦ç†é»„ç½‘ç‰‡ ä¼¦ç†é»„ç½‘ç”µå½± ä¼¦ç†é»„ç½‘è§†é¢‘ ä¼¦ç†é»„ç½‘è§†é¢‘è§‚çœ‹ ä¼¦ç†é»„è§†é¢‘ ä¼¦ç†é»„è§†é¢‘ç‰‡ ä¼¦ç†é»„è§†é¢‘ç½‘ ä¼¦ç†é»„è§†é¢‘è§‚çœ‹ ä¼¦ç†é»„è§†é¢‘é¢‘é“ ä¼¦ç†é»„é¢‘é“ ä½ ä¸ªå‚»æ¯” ä½ ä»–é©¬çš„ ä½ å…¨å®¶ ä½ å¥¶å¥¶çš„ ä½ å¥¹é©¬çš„ ä½ å¦ˆçš„ ä½ å¨˜å¡å¥½ ä½ å¨˜å’§ ä½ å®ƒå¦ˆçš„ ä½ å®ƒé©¬çš„ ä½ æ˜¯é¸¡ ä½ æ˜¯é¸­ ä½ é©¬çš„ åšçˆ± å‚»æ¯” å‚»é€¼ å…è´¹äººæˆè§†é¢‘ å…è´¹ä¼¦ç† å…è´¹ä¼¦ç†å½±ç‰‡ å…è´¹ä¼¦ç†è§†é¢‘ å…è´¹åœ¨çº¿ä¼¦ç†é»„å½±ç‰‡ å…è´¹åœ¨çº¿ä¼¦ç†é»„ç”µå½± å…è´¹åœ¨çº¿ä¼¦ç†é»„ç½‘å½±ç‰‡ å…è´¹åœ¨çº¿ä¼¦ç†é»„ç½‘ç”µå½± å…è´¹åœ¨çº¿ä¼¦ç†é»„ç½‘è§†é¢‘ å…è´¹åœ¨çº¿ä¼¦ç†é»„è§†é¢‘ å…è´¹åœ¨çº¿æˆäººè§†é¢‘ å…è´¹åœ¨çº¿æˆäººé»„å½±ç‰‡ å…è´¹åœ¨çº¿æˆäººé»„ç”µå½± å…è´¹åœ¨çº¿æˆäººé»„ç½‘å½±ç‰‡ å…è´¹åœ¨çº¿æˆäººé»„ç½‘ç”µå½± å…è´¹åœ¨çº¿æˆäººé»„ç½‘è§†é¢‘ å…è´¹åœ¨çº¿æˆäººé»„è§†é¢‘ å…è´¹åœ¨çº¿é»„å½±ç‰‡ å…è´¹åœ¨çº¿é»„ç”µå½± å…è´¹åœ¨çº¿é»„ç½‘å½±ç‰‡ å…è´¹åœ¨çº¿é»„ç½‘ç”µå½± å…è´¹åœ¨çº¿é»„ç½‘è§†é¢‘ å…è´¹åœ¨çº¿é»„è§†é¢‘ å…è´¹æˆäººç”µå½± å…è´¹æˆäººè§†é¢‘ å…è´¹æˆäººè§†é¢‘è§‚çœ‹ å…è´¹æ— ç  å…è´¹è‰²æƒ…å½±ç‰‡ å…è´¹è‰²æƒ…ç”µå½± å…è´¹è‰²æƒ…è§†é¢‘ å…è´¹è§†é¢‘åœ¨çº¿è§‚çœ‹ å…­åˆå½© å†Œé‚£ å†›å¦“ åˆ†åˆ†å½© åŒ—äº¬èµ›è½¦å¼€å¥– åˆå¤œç”µå½± åˆå¤œç¦åˆ© å–B å–æ¯” å–æ·« åšå½© å£äº¤ å£è‚¯ åƒå±Ž å¹ç®« å•ªå•ªå•ªè§†é¢‘ å›½äº§av å›½äº§ç²¾å“ å›½äº§è‡ªæ‹ å›½äº§è‡ªæ‹ç‰‡ å›½äº§è‡ªæ‹è§†é¢‘ åœ¨çº¿av åœ¨çº¿avåœ¨çº¿ åœ¨çº¿avç›´æ’­ åœ¨çº¿avç½‘å€ åœ¨çº¿avè§†é¢‘ åœ¨çº¿ä¼¦ç†é»„å½±ç‰‡ åœ¨çº¿ä¼¦ç†é»„ç‰‡ åœ¨çº¿ä¼¦ç†é»„ç”µå½± åœ¨çº¿ä¼¦ç†é»„ç½‘å½±ç‰‡ åœ¨çº¿ä¼¦ç†é»„ç½‘ç”µå½± åœ¨çº¿ä¼¦ç†é»„ç½‘è§†é¢‘ åœ¨çº¿ä¼¦ç†é»„è§†é¢‘ åœ¨çº¿å›½äº§ åœ¨çº¿å¤§é¦™è•‰ åœ¨çº¿æˆäººé»„å½±ç‰‡ åœ¨çº¿æˆäººé»„ç‰‡ åœ¨çº¿æˆäººé»„ç”µå½± åœ¨çº¿æˆäººé»„ç½‘å½±ç‰‡ åœ¨çº¿æˆäººé»„ç½‘ç”µå½± åœ¨çº¿æˆäººé»„ç½‘è§†é¢‘ åœ¨çº¿æˆäººé»„è§†é¢‘ åœ¨çº¿è§‚çœ‹ä¸­æ–‡å­—å¹• åœ¨çº¿è§‚çœ‹å…è´¹ åœ¨çº¿é»„å½±ç‰‡ åœ¨çº¿é»„ç‰‡ åœ¨çº¿é»„ç”µå½± åœ¨çº¿é»„ç½‘å½±ç‰‡ åœ¨çº¿é»„ç½‘ç”µå½± åœ¨çº¿é»„ç½‘è§†é¢‘ åœ¨çº¿é»„è§†é¢‘ å¡žä½ å…¬ å¡žä½ å¨˜ å¡žä½ æ¯ å¡žä½ çˆ¸ å¡žä½ è€å¸ˆ å¡žä½ è€æ¯ å¤œå¤œå•ªè§†é¢‘åœ¨çº¿è§‚çœ‹ å¤§åµå­ å¤§åµæ³¡ å¤§å‘äº‘ å¤§å‘å®˜ç½‘ å¤§å‘å½©ç¥¨ å¤§å‘å½©ç¥¨å®˜ç½‘ å¤§å‘å¿«ä¸‰ å¤§å‘å¿«ä¸‰å’Œå€¼ å¤§å‘å¿«ä¸‰å¤§å°å•åŒ å¤§å‘å¿«ä¸‰å¦‚ä½• å¤§å‘å¿«ä¸‰å®˜ç½‘ å¤§å‘å¿«ä¸‰å¼€å¥– å¤§å‘å¿«ä¸‰å¼€å¥–ç»“æžœ å¤§å‘å¿«ä¸‰æ€Žä¹ˆ å¤§å‘å¿«ä¸‰æ€Žä¹ˆçœ‹ å¤§å‘å¿«ä¸‰æ˜¯ä¸æ˜¯ å¤§å‘å¿«ä¸‰æ˜¯ä»€ä¹ˆ å¤§å‘å¿«ä¸‰æ˜¯å›½å®¶ å¤§å‘å¿«ä¸‰è®¡åˆ’ å¤§å‘å¿«ä¸‰èµ°åŠ¿å›¾ å¤§å‘æ‰‘å…‹ å¤§å‘æ—¶æ—¶å½© å¤§å‘æ—¶æ—¶å½©å¼€å¥– å¤§å‘æ—¶æ—¶å½©æ˜¯ å¤§å‘æ—¶æ—¶å½©è®¡åˆ’ å¤§å‘æ£‹ç‰Œ å¤§å‘æ¸¸æˆ å¤§å‘æ¸¸æˆå®˜ç½‘ å¤§é¦™è•‰ å¤§é¦™è•‰ä¼Šäºº å¤§é¸¡å·´ å¤©å¤©ä¸­å½©ç¥¨ å¤©å¤©ä¸­å½©ç¥¨app å¤©å¤©ä¸­å½©ç¥¨å¾®ä¿¡ å¤©å¤©ä¸­å½©ç¥¨æ€Žä¹ˆ å¤©å¤©ä¸­å½©ç¥¨æ˜¯ å¤©å¤©ä¸­å½©ç¥¨çš„ å¤©å¤©å•ª å¤©å¤©å•ªåœ¨çº¿è§†é¢‘ å¤©å¤©å½©ç¥¨ å¤©å¤©å½©ç¥¨ç½‘ å¤©å¤©çˆ±å½©ç¥¨ å¤©å¤©èµ¢å½©ç¥¨ å¤«å¦»æ€§ç”Ÿæ´» å¥¸ä½  å¥¹å¦ˆåœ° å¥¹å¦ˆçš„ å¥¹é©¬çš„ å¦ˆB å¦ˆä¸ªB å¦ˆä¸ªæ¯” å¦ˆä¸ªè€æ¯” å¦ˆå¦ˆçš„ å¦ˆæ¯” å¦ˆçš„ å¦ˆçš„B å¦ˆé€¼ å¦“ å¦“å¥³ å¦³å¥¹å¦ˆçš„ å¦³å¦ˆçš„ å¦³å¨˜çš„ å¦³è€æ¯çš„ å¦³é©¬çš„ å¨±ä¹å¹³å°å¼€æˆ· å¨±ä¹å½©ç¥¨ å¼€å¥–ç»“æžœ å½©ç¥žäº‰éœ¸ å½©ç¥žäº‰éœ¸å¤§å‘å¿«ä¸‰ å½©ç¥žäº‰éœ¸æ€Žä¹ˆ å½©ç¥žäº‰éœ¸æ˜¯ å½©ç¥žäº‰éœ¸ç½‘ç«™ å½©ç¥žäº‰éœ¸é‚€è¯·ç  å½©ç¥¨ å½©ç¥¨å¤©å¤© å½©ç¥¨å¨±ä¹ å½©ç¥¨å¨±ä¹æ³¨å†Œ å½©ç¥¨å¹³å° å½©ç»å½©ç¥¨ å¾®ä¿¡çš„å¤©å¤©ä¸­å½©ç¥¨ æ€§äº¤ æ€§æ„Ÿå†™çœŸ æ€§æ„Ÿå†™çœŸè§†é¢‘ æ€§æ„Ÿç¾Žå¥³ æ€§æ„Ÿè§†é¢‘ æ€§çˆ± æ€§çˆ±è§†é¢‘ æƒ…è‰²å›¾ç‰‡ æƒ…è‰²ç”µå½± æƒ…è‰²ç½‘ç«™ æƒ…è‰²è§†é¢‘ æƒ…è‰²è®ºå› æˆäººä¸‹è½½ æˆäººä¹¦åˆŠ æˆäººåŠ¨æ¼« æˆäººåŠ¨æ¼«ç½‘ æˆäººå›¾ç‰‡ æˆäººåœ¨çº¿å½±ç‰‡ æˆäººåœ¨çº¿ç”µå½± æˆäººåœ¨çº¿ç½‘ æˆäººåœ¨çº¿è§†é¢‘ æˆäººåœ¨çº¿é»„å½±ç‰‡ æˆäººåœ¨çº¿é»„ç”µå½± æˆäººåœ¨çº¿é»„ç½‘å½±ç‰‡ æˆäººåœ¨çº¿é»„ç½‘ç”µå½± æˆäººåœ¨çº¿é»„ç½‘è§†é¢‘ æˆäººåœ¨çº¿é»„è§†é¢‘ æˆäººå°è¯´ æˆäººå½±åŸŽ æˆäººå½±ç‰‡ æˆäººå½±ç‰‡è§‚çœ‹ æˆäººå½±è§† æˆäººæ–‡å­¦ æˆäººæ–‡å­¦ç½‘ æˆäººæ¼«ç”» æˆäººç‰‡ æˆäººç”µå½± æˆäººç”µå½±ä¸‹è½½ æˆäººç”µå½±å…è´¹ æˆäººç”µå½±åœ¨çº¿ æˆäººç”µå½±æ’­æ”¾ æˆäººç”µå½±ç‰‡ æˆäººç”µå½±ç‰‡è§‚çœ‹ æˆäººç”µå½±è§‚çœ‹ æˆäººç”µå½±è§‚çœ‹å…è´¹ æˆäººç”µå½±è§‚çœ‹å¹³å° æˆäººç”µå½±è§‚çœ‹ç½‘ æˆäººç”µå½±è§‚çœ‹ç½‘å€ æˆäººç”µè§† æˆäººç§€ æˆäººç½‘ç«™ æˆäººè‡ªæ‹ æˆäººè§†é¢‘ æˆäººè§†é¢‘å…è´¹ç½‘ç«™ æˆäººè§†é¢‘å•ªå•ªå•ª æˆäººè§†é¢‘åœ¨çº¿ æˆäººè§†é¢‘åœ¨çº¿è§‚çœ‹ æˆäººè§†é¢‘å¹³å° æˆäººè§†é¢‘æ’­æ”¾ æˆäººè§†é¢‘ç‰‡ æˆäººè§†é¢‘ç§€ æˆäººè§†é¢‘ç½‘ æˆäººè§†é¢‘è‡ªæ‹ æˆäººè§†é¢‘è§‚çœ‹ æˆäººè§†é¢‘è§‚çœ‹å…è´¹ æˆäººè§†é¢‘è§‚çœ‹å¹³å° æˆäººè§†é¢‘è§‚çœ‹ç½‘ç«™ æˆäººè§†é¢‘é¢‘é“ æˆäººè®ºå› æˆäººé¢‘é“ æˆäººé»„å½±ç‰‡ æˆäººé»„å½±ç‰‡è§‚çœ‹ æˆäººé»„ç‰‡ æˆäººé»„ç‰‡ç½‘ç«™ æˆäººé»„ç”µå½± æˆäººé»„ç”µå½±ç‰‡ æˆäººé»„ç”µå½±è§‚çœ‹ æˆäººé»„ç”µè§† æˆäººé»„ç”µè§†ç‰‡ æˆäººé»„ç½‘å½±ç‰‡ æˆäººé»„ç½‘ç‰‡ æˆäººé»„ç½‘ç”µå½± æˆäººé»„ç½‘è§†é¢‘ æˆäººé»„ç½‘è§†é¢‘è§‚çœ‹ æˆäººé»„è§†é¢‘ æˆäººé»„è§†é¢‘ç‰‡ æˆäººé»„è§†é¢‘ç½‘ æˆäººé»„è§†é¢‘è§‚çœ‹ æˆäººé»„è§†é¢‘é¢‘é“ æˆäººé»„é¢‘é“ æ— ç  æ— ç av æ— ç ä¸€åŒºäºŒåŒºä¸‰åŒº æ— ç ä¸å¡é«˜æ¸…å…è´¹ æ— ç ä¸å¡é«˜æ¸…å…è´¹v æ— ç ä¸­æ–‡å­—å¹• æ— ç å›½äº§è‡ªæ‹ æ—¥æœ¬ä¸€æœ¬é“ æ—¥æœ¬æ¯›ç‰‡å…è´¹è§†é¢‘è§‚çœ‹ æ—¥éŸ©av æ—¶æ—¶å½© æ—¶æ—¶å½©è®¡åˆ’ æ·«ç§½ æ¿€æƒ…å°è¯´ æ¿€æƒ…è£¸èŠ æ¿€æƒ…è£¸èˆž æ¿€æƒ…è§†é¢‘ çƒ­ä¹…ä¹…ç²¾å“ çƒ­åœ¨çº¿ç²¾å“ çƒ­è¿™é‡Œåªæœ‰ç²¾å“ ç”·äººçš„å¤©å ‚ çœŸäººæ€§ç”Ÿæ´»ç›´æ’­ çœŸäººæ€§ç”Ÿæ´»è§†é¢‘ çœŸäººæ€§ç›´æ’­ çœŸäººæ€§è¡Œä¸ºç›´æ’­ çœŸäººæ€§è¡Œä¸ºè§†é¢‘ çœŸäººæ€§è§†é¢‘ çœŸäººç§€ çœŸäººç§€è§†é¢‘ çœŸäººè‰²æƒ… çœŸäººè‰²æƒ…è§†é¢‘ çœŸäººè£¸èŠ çœŸäººè£¸èˆž çœŸäººè§†é¢‘ çœŸäººè§†é¢‘ç§€ ç¥žå½©äº‰éœ¸ ç¦åˆ©å½©ç¥¨ ç¦åˆ©è§†é¢‘ ç²¾å“ä¸€åŒºäºŒåŒºä¸‰åŒº ç²¾å“å›½äº§ ç¾Žå¥³è£¸èŠ ç¾Žå¥³è£¸èˆž è‡ªæ‹è§†é¢‘ è‰²æƒ… è‰²æƒ…å›¾ç‰‡ è‰²æƒ…å½±ç‰‡ è‰²æƒ…ç”µå½± è‰²æƒ…ç”µå½±è§‚çœ‹ è‰²æƒ…ç”µè§† è‰²æƒ…ç½‘ è‰²æƒ…ç½‘ç«™ è‰²æƒ…è§†é¢‘ è‰²æƒ…è§†é¢‘åœ¨çº¿ è‰²æƒ…è§†é¢‘åœ¨çº¿è§‚çœ‹ è‰²æƒ…è§†é¢‘ç½‘ è‰²æƒ…è§†é¢‘é¢‘é“ è‰²æƒ…é¢‘é“ è‰²ç”µå½± è‰²ç½‘ è‰²ç½‘ç«™ è‰²è§†é¢‘ è‰²è§†é¢‘åœ¨çº¿ è‰²è§†é¢‘ç½‘ç«™ èµŒ èµŒå€º èµŒåš èµŒåŸŽ èµŒå±€ èµŒå¾’ èµŒæ¡Œ èµŒæ³¨ èµŒçŽ‹ èµŒç˜¾ èµŒç›˜ èµŒèµ› èµŒé’± èµŒé¬¼ é‡åº†æ—¶æ—¶å½© é‡åº†æ—¶æ—¶å½©æ€ éœ²ç‚¹ é«˜æ¸…æ— ç  é»„å½±ç‰‡ é»„ç‰‡ é»„ç‰‡ç½‘ç«™ é»„ç”µå½± é»„ç”µå½±ç‰‡ é»„ç”µè§† é»„ç”µè§†ç‰‡ é»„ç½‘ é»„ç½‘å½±ç‰‡ é»„ç½‘ç‰‡ é»„ç½‘ç”µå½± é»„ç½‘è§†é¢‘ é»„è‰² é»„è‰²å½•åƒ é»„è‰²å½•åƒå½±ç‰‡ é»„è‰²å½•åƒç‰‡ é»„è‰²å½•åƒç‰‡å½±ç‰‡ é»„è‰²å½•åƒç‰‡ç”µå½± é»„è‰²å½•åƒç‰‡ç½‘å½±ç‰‡ é»„è‰²å½•åƒç‰‡ç½‘ç”µå½± é»„è‰²å½•åƒç‰‡ç½‘è§†é¢‘ é»„è‰²å½•åƒç‰‡è§†é¢‘ é»„è‰²å½•åƒç”µå½± é»„è‰²å½•åƒç½‘å½±ç‰‡ é»„è‰²å½•åƒç½‘ç”µå½± é»„è‰²å½•åƒç½‘è§†é¢‘ é»„è‰²å½•åƒè§†é¢‘ é»„è§†é¢‘ é»„è§†é¢‘ç‰‡ é»„è§†é¢‘ç½‘\".split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "POLICY_SUBSTRINGS = [\n",
    "    \"terms of use\",\n",
    "    \"privacy policy\",\n",
    "    \"cookie policy\",\n",
    "    \"uses cookies\",\n",
    "    \"use of cookies\",\n",
    "    \"use cookies\",\n",
    "]\n",
    "\n",
    "\n",
    "def list_case_filter(sentence):\n",
    "    tokens = sentence.split()\n",
    "    capital_tokens = [token for token in tokens if token[0].isupper() or all(char.isdigit() or char in string.punctuation for char in token)]\n",
    "    warning = len(tokens) >= 12 and (len(capital_tokens) / len(tokens)) > 0.5\n",
    "    return warning\n",
    "\n",
    "\n",
    "def danger_chars_filter(sentence):\n",
    "    danger_chars_count = sum(1 for char in sentence if char in '0123456789{}+/()>')\n",
    "    warning = (danger_chars_count / len(sentence)) > 0.2\n",
    "    return warning\n",
    "\n",
    "\n",
    "def cursedness_filter(sentence):\n",
    "    warning = any(curse in sentence for curse in CURSED_SUBSTRINGS)\n",
    "    return warning\n",
    "\n",
    "def adult_filter(sentence):\n",
    "    warning = any(curse in sentence for curse in ADULT_SIGNALS + EXTRA_ADULT_SGINALS)\n",
    "    return warning\n",
    "\n",
    "def detect_long_words(sentence, max_chars=100):\n",
    "    words = sentence.split()\n",
    "    long_words = [word for word in words if len(word) > max_chars]\n",
    "    return len(long_words) > 0\n",
    "\n",
    "def detect_js_warning(sentence):\n",
    "\n",
    "    # Check if \"Javascript\" is present in the text\n",
    "    if 'Javascript' in sentence or 'JavaScript' in sentence:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_lorem_ipsum(sentence):\n",
    "    # Convert text to lowercase to make the search case-insensitive\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Check if the phrase \"lorem ipsum\" is present in the text\n",
    "    if \"lorem ipsum\" in sentence:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_curly_bracket(sentence):\n",
    "    # Check if the curly bracket '{' is present in the text\n",
    "    if \"{\" in sentence:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_policy(text):\n",
    "    text_lower = text.lower()\n",
    "    for substring in POLICY_SUBSTRINGS:\n",
    "        if substring in text_lower:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GopherRepetitionFilter():\n",
    "    \n",
    "    \"\"\"Check if there is repeated content in the input text. Excessive\n",
    "    repetition is often linked with uninformative content and can be used to\n",
    "    determine whether it is low-quality text. This function implements\n",
    "    \"Repetition Removal\" as described in Gopher_.\n",
    "\n",
    "    .. _Gopher: https://arxiv.org/abs/2112.11446\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_n_grams(words: List[str], n: int) -> List[str]:\n",
    "        return [\" \".join(words[i : i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "    @staticmethod\n",
    "    def find_top_duplicate(x: List[str]) -> int:\n",
    "        counter = Counter()\n",
    "        for element in x:\n",
    "            counter[element] += 1\n",
    "        top_n_gram = counter.most_common(1)[0]\n",
    "        return len(top_n_gram[0]) * top_n_gram[1]\n",
    "\n",
    "\n",
    "    def filter(self, text, top_n_grams):\n",
    "        \n",
    "        text = self.add_space_between_numbers(text)\n",
    "        words = text.split(' ')\n",
    "\n",
    "        for n, n_frac in top_n_grams:\n",
    "            n_grams = self.get_n_grams(words, n)\n",
    "            if not n_grams:\n",
    "                continue\n",
    "            top_char_length = self.find_top_duplicate(n_grams)\n",
    "            if top_char_length / len(text) > n_frac:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_space_between_numbers(numbers):\n",
    "        result = ''\n",
    "        i = 0\n",
    "        while i < len(numbers):\n",
    "            if numbers[i].isdigit():\n",
    "                result += numbers[i]\n",
    "                i += 1\n",
    "                while i < len(numbers) and numbers[i].isdigit():\n",
    "                    result += numbers[i]\n",
    "                    i += 1\n",
    "                if i < len(numbers) and not numbers[i].isdigit():\n",
    "                    result += ' '\n",
    "            else:\n",
    "                result += numbers[i]\n",
    "                if i+1 < len(numbers) and numbers[i+1].isdigit():\n",
    "                    result += ' '\n",
    "                i += 1\n",
    "        return result\n",
    "    \n",
    "    def filter_para(self, text):\n",
    "        \n",
    "        return self.filter(text, ((2, 0.2), (3, 0.18), (4, 0.16)))\n",
    "    \n",
    "    def filter_sent(self, text):\n",
    "        \n",
    "        sents = [s for s in text.split('\\n') if len(s.split(' ')) > 20]\n",
    "        warning = any([self.filter(s, ((1, 0.5), (2, 0.3))) for s in sents])\n",
    "        \n",
    "        return warning\n",
    "\n",
    "gopher_repetition = GopherRepetitionFilter().filter_para\n",
    "gopher_repetition_sent = GopherRepetitionFilter().filter_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filters = Filters()\n",
    "filters.add_filter(list_case_filter, \"list_case\")\n",
    "filters.add_filter(danger_chars_filter, \"danger_chars\")\n",
    "filters.add_filter(cursedness_filter, \"cursed_regex\")\n",
    "filters.add_filter(adult_filter, \"adult_signals\")\n",
    "filters.add_filter(detect_long_words, \"long_word\")\n",
    "filters.add_filter(detect_policy, \"detect_policy\")\n",
    "filters.add_filter(gopher_repetition, \"repetition\")\n",
    "filters.add_filter(gopher_repetition_sent, \"repetition_sent\")\n",
    "filters.add_filter(detect_js_warning, \"js_warning\")\n",
    "filters.add_filter(detect_lorem_ipsum, \"lorem_ipsum\")\n",
    "filters.add_filter(detect_curly_bracket, \"curly_bracket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DomainLabeler:\n",
    "    def __init__(self):\n",
    "        self.domain_groups = {}\n",
    "    \n",
    "    def add_domain_group(self, label, domains):\n",
    "        self.domain_groups[label] = domains\n",
    "    \n",
    "    def apply(self, uri, categories):\n",
    "        domain = urlparse(uri).netloc\n",
    "        for label, domains in self.domain_groups.items():\n",
    "            if any(domain.endswith(d) for d in domains):\n",
    "                categories.append(label)\n",
    "                break\n",
    "        return categories\n",
    "\n",
    "\n",
    "labeler = DomainLabeler()\n",
    "labeler.add_domain_group(\"religious\", [\"bible.com\", \"ebible.org\", \"png.bible\", \"jw.org\", \"wol.jw.org\", \"breakeveryyoke.com\", \"scriptureearth.org\", \"live.bible.is\", \"bible.is\", \"faithcomesbyhearing.com\", \"download.sabda.org\", \"sabda.org\", \"alkitab.mobi\", \"biblerevelation.org\", \"gospelgo.com\", \"mykitabsuci.org\", \"aboriginalbibles.org.au\", \"wikiislam.net\", \"stepbible.org\", \"e-alkitab.org\"])\n",
    "labeler.add_domain_group(\"wikipedia\", [\"wikipedia.org\", \"wikimedia.org\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class PIIReplacer:\n",
    "    def __init__(\n",
    "        self, regex: str, replacements, validator\n",
    "    ):\n",
    "        self.regex: re.Pattern = re.compile(regex)\n",
    "        self.replacements = (\n",
    "            replacements\n",
    "            if type(replacements) is tuple\n",
    "            else (tuple(replacements) if not isinstance(replacements, str) else (replacements,))\n",
    "        )\n",
    "        self.validator = validator  # extra validation for a match\n",
    "        self._replace_i = 0\n",
    "\n",
    "    def replace(self, text: str):\n",
    "        def get_replacement(matchobj):\n",
    "            if self.validator and not self.validator(matchobj.group(0)):\n",
    "                # not a valid match. replace with itself\n",
    "                return matchobj.group(0)\n",
    "            replacement = self.replacements[self._replace_i]\n",
    "            self._replace_i = (self._replace_i + 1) % len(self.replacements)\n",
    "            return replacement\n",
    "\n",
    "        return self.regex.sub(get_replacement, text)\n",
    "\n",
    "\n",
    "def public_ip_validator(ip, public_only: bool = True) -> bool:\n",
    "    try:\n",
    "        ip = ipaddress.ip_address(ip)\n",
    "        return not public_only or ip.is_global\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "class PIIFormatter():\n",
    "    \"\"\"\n",
    "    Replaces email addresses and ip addresses in the document text.\n",
    "    Args:\n",
    "        remove_emails: Replace email addresses\n",
    "        remove_ips: Replace IP addresses\n",
    "        only_remove_public_ips: by default we only replace public (and thus PII) IPs\n",
    "        email_replacement: tuple of strings to use as replacement. They will be used in a circular way\n",
    "        ip_replacement same as email_replacement but for IP addresses\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"ðŸ“ž PII\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        remove_emails: bool = True,\n",
    "        remove_ips: bool = True,\n",
    "        only_remove_public_ips: bool = True,\n",
    "        # example.com/org are actually maintained as an example\n",
    "        email_replacement  = (\"email@example.com\", \"firstname.lastname@example.org\"),\n",
    "        # randomly generated list of ips. they did not respond to ping requests at the time the list was created\n",
    "        ip_replacement = (\n",
    "            \"22.214.171.124\",\n",
    "            \"126.96.36.199\",\n",
    "            \"188.8.131.52\",\n",
    "            \"184.108.40.206\",\n",
    "            \"220.127.116.11\",\n",
    "            \"18.104.22.168\",\n",
    "        ),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.remove_emails = remove_emails\n",
    "        self.remove_ips = remove_ips\n",
    "\n",
    "        self.emails_replacer = PIIReplacer(\n",
    "            r\"\\b[A-Za-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[A-Za-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:(?:[A-Za-z0-9](?:[\"\n",
    "            r\"A-Za-z0-9-]*[A-Za-z0-9])?\\.)+[A-Za-z0-9](?:[A-Za-z0-9-]*[A-Za-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[\"\n",
    "            r\"01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[A-Za-z0-9-]*[A-Za-z0-9]:)])\",\n",
    "            email_replacement,\n",
    "            None\n",
    "        )\n",
    "\n",
    "        self.ip_replacer = PIIReplacer(\n",
    "            r\"(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\",\n",
    "            validator=partial(public_ip_validator, public_only=only_remove_public_ips),\n",
    "            replacements=ip_replacement,\n",
    "        )\n",
    "\n",
    "    def format(self, text: str) -> str:\n",
    "        if self.remove_emails:\n",
    "            text = self.emails_replacer.replace(text)\n",
    "        if self.remove_ips:\n",
    "            text = self.ip_replacer.replace(text)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "pii_format = PIIFormatter().format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def detect_script(text, lang):\n",
    "    \n",
    "    main_script, percentage, details = sp(text)\n",
    "    if lang == 'jpn':\n",
    "        main_script = 'Jpan'\n",
    "        percentage = details['details'].get('Hani', 0) + details['details'].get('Hira', 0) + details['details'].get('Kana', 0)\n",
    "        \n",
    "    return main_script, percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_json(json_data, lang, script):\n",
    "    \n",
    "    # pii\n",
    "    json_data[\"content\"] = pii_format(json_data[\"content\"])\n",
    "    content = json_data[\"content\"]\n",
    "    \n",
    "    warc_headers = json_data[\"warc_headers\"]\n",
    "    uri = warc_headers[\"warc-target-uri\"]\n",
    "\n",
    "    metadata = json_data[\"metadata\"]\n",
    "    \n",
    "    quality_warnings = metadata.get(\"quality_warnings\", [])\n",
    "    categories = metadata.get(\"categories\", [])\n",
    "    \n",
    "    if categories is None:\n",
    "        categories = []\n",
    "\n",
    "    if quality_warnings is None:\n",
    "        quality_warnings = []\n",
    "        \n",
    "    quality_warnings = filters.apply_filters(content, quality_warnings)\n",
    "    categories = labeler.apply(uri, categories)\n",
    "\n",
    "    metadata[\"quality_warnings\"] = sorted(set(quality_warnings))\n",
    "    metadata[\"categories\"] = sorted(set(categories))\n",
    "\n",
    "    \n",
    "    # run script identification\n",
    "    script_label, script_percentage = detect_script(content, lang)\n",
    "    metadata[\"script\"] = {\"label\": script_label, \"percentage\": float(\"{:.2f}\".format(script_percentage))}\n",
    "\n",
    "    \n",
    "    ## lang identification consistency\n",
    "    \n",
    "    label = metadata['identification']['label']\n",
    "    sent_labels = [int(isent['label']==label) for isent in metadata['sentence_identifications'] if isinstance(isent, dict)]\n",
    "    metadata['identification_consistency'] = {\"percentage\": float(\"{:.2f}\".format(sum(sent_labels)/len(sent_labels))), 'num_sents': len(sent_labels)}\n",
    "    \n",
    "    \n",
    "    return json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Hani_nonspace_percentage(text):\n",
    "    # Split the text into words\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    # Count the number of words with 10 or more characters\n",
    "    long_words_length = sum(len(word) for word in words if len(word) >= 20)\n",
    "    \n",
    "    # Calculate the percentage\n",
    "    percentage = long_words_length / len(text)\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def post_annotation_filter(json_data, lang, script):\n",
    "    \n",
    "    metadata = json_data['metadata']\n",
    "    \n",
    "    if 'wikipedia' in metadata[\"categories\"]:\n",
    "        quality = set(metadata['quality_warnings']) - set([\"short_sentences\",\"header\", \"footer\", \"tiny\", \"long_word\", \"repetition\", \"repetition_sent\"])\n",
    "    \n",
    "    # tiny for oscar is based on 5 sentences, we decrease it to 3\n",
    "    if 'tiny' in metadata['quality_warnings']:\n",
    "        \n",
    "        if metadata['identification_consistency']['num_sents'] >= 3:\n",
    "            metadata['quality_warnings'].remove('tiny')\n",
    "    \n",
    "    \n",
    "    if metadata['script']['label'] != script and script not in ['Hani', 'Hans', 'Hant']:\n",
    "        return False\n",
    "    \n",
    "    if script in ['Hani', 'Hans', 'Hant'] and metadata['script']['label'] not in ['Hani', 'Hans', 'Hant']:\n",
    "        return False\n",
    "    \n",
    "    if metadata['script']['percentage'] < 0.9:\n",
    "        return False\n",
    "    \n",
    "    if lang != 'und' and metadata['identification_consistency']['percentage'] < 0.6:\n",
    "        return False\n",
    "\n",
    "    if script in ['Hani', 'Jpan']:\n",
    "        \n",
    "        if Hani_nonspace_percentage(json_data['content']) < 0.3:\n",
    "            metadata['quality_warnings'].append('hani_list_case')\n",
    "          \n",
    "        \n",
    "        # for chinese and japanse do not remove tiny. \n",
    "        quality = set(metadata['quality_warnings']) - set([\"short_sentences\",\"header\", \"footer\", \"tiny\", \"long_word\", \"repetition\", \"repetition_sent\"])\n",
    "\n",
    "\n",
    "    if script in ['Latn', 'Cyrl', 'Arab', 'Grek', 'Hebr', 'Deva', 'Beng']:\n",
    "        quality = set(metadata['quality_warnings']) - set([\"short_sentences\",\"header\", \"footer\"])\n",
    "\n",
    "    else:\n",
    "        quality = set(metadata['quality_warnings']) - set([\"short_sentences\",\"header\", \"footer\", \"long_word\", \"repetition\", \"repetition_sent\"])\n",
    "    \n",
    "    \n",
    "    # quality = quality - set(['curly_bracket'])\n",
    "    \n",
    "    if len(quality)!=0:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Batch size\n",
    "batch_size = 2000\n",
    "\n",
    "\n",
    "# Function to process JSON data\n",
    "def process_json_file(file_path):\n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    filter_file_path = os.path.join(filter_dir, os.path.basename(file_path))\n",
    "\n",
    "    lang =  os.path.basename(file_path).split('-')[0]\n",
    "    script = os.path.basename(file_path).split('-')[-1].split('_')[0]\n",
    "    \n",
    "    if lang == 'multi':\n",
    "        return None\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as input_file, open(output_file_path, 'w', encoding='utf-8') as output_file, open(filter_file_path, 'w', encoding='utf-8') as filter_file:\n",
    "        batch = []\n",
    "        batch_filter = []\n",
    "        for line in tqdm(input_file, desc=f'Processing {file_path}'):\n",
    "            # Parse the line as JSON\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except:\n",
    "                continue \n",
    "            # Process the JSON object\n",
    "            processed_data = process_json(data, lang, script)\n",
    "            # Add processed data to the batch\n",
    "            batch.append(processed_data)\n",
    "\n",
    "            if post_annotation_filter(processed_data, lang, script):\n",
    "                batch_filter.append(processed_data)\n",
    "\n",
    "            # If the batch is full, write it to the output file\n",
    "            if len(batch) == batch_size:\n",
    "                for item in batch:\n",
    "                    output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                batch = []\n",
    "\n",
    "            # If the batch_filter is full, write it to the output file\n",
    "            if len(batch_filter) == batch_size:\n",
    "                for item in batch_filter:\n",
    "                    filter_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                batch_filter = []\n",
    "\n",
    "        # Write remaining data in the last batch to the output file\n",
    "        for item in batch:\n",
    "            output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        # Write remaining data in the last filter_batch to the output file\n",
    "        for item in batch_filter:\n",
    "            filter_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_cores = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to filter paths based on file size\n",
    "def filter_paths_by_size(paths, max_size_mb):\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n",
    "    filtered_paths = [path for path in paths if os.path.isfile(path) and os.path.getsize(path) < max_size_bytes]\n",
    "    return filtered_paths\n",
    "\n",
    "input_dir = 'res/corpus/'\n",
    "output_dir = 'res/annotation/'\n",
    "filter_dir = 'res/filter/'\n",
    "\n",
    "# Get list of input files\n",
    "input_files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.jsonl')]\n",
    "input_files = sorted(input_files, key=os.path.getsize)\n",
    "\n",
    "input_files_100mgb = filter_paths_by_size(input_files, 100)\n",
    "\n",
    "rest_input_files = list(set(input_files) - set(input_files_100mgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of processes to run in parallel\n",
    "num_processes = min(len(input_files_100mgb), num_cores)\n",
    "\n",
    "# Create a pool of processes\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    # Map the processing function to each input file\n",
    "    pool.map(process_json_file, input_files_100mgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "\n",
    "def process_batch(lines, lang, script):\n",
    "    \n",
    "    batch = []\n",
    "    batch_filter = []\n",
    "    for line in lines:\n",
    "        # Parse the line as JSON\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except:\n",
    "            continue \n",
    "        # Process the JSON object\n",
    "        processed_data = process_json(data, lang, script)\n",
    "        # Add processed data to the batch\n",
    "        batch.append(processed_data)\n",
    "\n",
    "        if post_annotation_filter(processed_data, lang, script):\n",
    "            batch_filter.append(processed_data)\n",
    "            \n",
    "    return batch, batch_filter\n",
    "\n",
    "\n",
    "def check_and_remove_empty_file(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file at file_path is empty and remove it if it is.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the file to be checked and potentially removed.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path) and os.path.getsize(file_path) == 0:\n",
    "        os.remove(file_path)\n",
    "\n",
    "\n",
    "def process_chunk(start_line, end_line, input_file_path, chunk_index, batch_size=2000):\n",
    "    \n",
    "    print(\"chunk_index\", chunk_index)\n",
    "    \n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(input_file_path).replace('.jsonl', f'_{chunk_index}.jsonl'))\n",
    "    filter_file_path = os.path.join(filter_dir, os.path.basename(input_file_path).replace('.jsonl', f'_{chunk_index}.jsonl'))\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile, open(filter_file_path, 'w', encoding='utf-8') as filterfile:\n",
    "        \n",
    "        lang =  os.path.basename(input_file_path).split('-')[0]\n",
    "        script = os.path.basename(input_file_path).split('-')[-1].split('_')[0]\n",
    "\n",
    "        \n",
    "        current_line = 0\n",
    "        lines = []\n",
    "        \n",
    "        for line in infile:\n",
    "            if current_line >= start_line:\n",
    "                lines.append(line)\n",
    "            current_line += 1\n",
    "\n",
    "            if current_line >= end_line:\n",
    "                break\n",
    "\n",
    "            if len(lines) == batch_size:\n",
    "                # Process the lines (replace this with your actual processing function)\n",
    "                processed_lines, processed_filtered_lines  = process_batch(lines, lang, script)\n",
    "                                \n",
    "                for item in processed_lines:\n",
    "                    outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                for item in processed_filtered_lines:\n",
    "                    filterfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                lines = []\n",
    "\n",
    "        # Process any remaining lines in the last batch\n",
    "        if lines:\n",
    "            processed_lines, processed_filtered_lines  = process_batch(lines, lang, script)\n",
    "\n",
    "            for item in processed_lines:\n",
    "                outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            for item in processed_filtered_lines:\n",
    "                filterfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "    check_and_remove_empty_file(output_file_path)\n",
    "    check_and_remove_empty_file(filter_file_path)\n",
    "    \n",
    "    return True\n",
    "        \n",
    "        \n",
    "        \n",
    "# def count_lines(file_path):\n",
    "#     result = subprocess.run(['sed', '-n', '$=', file_path], capture_output=True, text=True)\n",
    "#     return int(result.stdout.strip())\n",
    "              \n",
    "\n",
    "def count_lines(filename):\n",
    "    count = 0\n",
    "    with open(filename, 'rb') as f:\n",
    "        while chunk := f.read(1024*1024*1024):\n",
    "            count += chunk.count(b'\\n')\n",
    "    return count        \n",
    "\n",
    "def chunkify(file_path, num_chunks):\n",
    "    # with open(file_path, 'r') as infile:\n",
    "    #    total_lines = sum(1 for _ in infile)\n",
    "    print(file_path)\n",
    "    total_lines = count_lines(file_path)\n",
    "    chunk_size = total_lines // num_chunks\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start_line = i * chunk_size\n",
    "        end_line = start_line + chunk_size if i != num_chunks - 1 else total_lines\n",
    "        chunks.append((start_line, end_line))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def parallel(input_file, num_cores):\n",
    "    \n",
    "    print(\"start\", input_file)\n",
    "    chunks = chunkify(input_file, num_cores)\n",
    "    print(\"chunks\", len(chunks))\n",
    "    \n",
    "    processes = []\n",
    "    for chunk_index, (start_line, end_line) in enumerate(chunks):\n",
    "        p = mp.Process(target=process_chunk, args=(start_line, end_line, input_file, chunk_index))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rest_input_files = sorted(rest_input_files, key=os.path.getsize)\n",
    "\n",
    "for r in rest_input_files:\n",
    "    parallel(r, num_cores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lid",
   "language": "python",
   "name": "lid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}